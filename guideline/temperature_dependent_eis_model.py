# -*- coding: utf-8 -*-
"""Temperature Dependent EIS model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OhqgaO87q1AUKkwD6RQAVtMNVW5UGmgt

# **Construction of Synthetic Data**
"""

import numpy as np
import pandas as pd
from scipy.optimize import curve_fit
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

"""**Define the temperature dependent fitting function**"""

def fit_function(temp, *coefficients):
    return sum(c * temp**i for i, c in enumerate(coefficients))

"""**Define a function to fit the data and print the equation with validation score**"""

def fit_and_print_equation(data, temp_column, param_column):
    temp_data = data[temp_column]
    param_data = data[param_column]

    # Perform curve fitting with a higher degree polynomial and a better initial guess

    degree = 10  # Adjust the degree as needed
    initial_guess = [0.0] * (degree + 1)  # Initial guess for coefficients
    coefficients, _ = curve_fit(fit_function, temp_data, param_data, p0=initial_guess)

    # Print the equation

    equation = " + ".join([f"{round(c, 4)} * temp**{i}" if i > 0 else f"{round(c, 4)}" for i, c in enumerate(coefficients)])
    print(f"Equation for {param_column} in terms of temperature: {equation}")

    # Validate the fit by plotting

    plt.scatter(temp_data, param_data, label="Original Data")

    # Plot the fitted curve

    temp_values = np.linspace(min(temp_data), max(temp_data), 100)
    fitted_curve = fit_function(temp_values, *coefficients)
    plt.plot(temp_values, fitted_curve, color='red', label="Fitted Curve")

    plt.xlabel('Temperature')
    plt.ylabel(param_column)
    plt.legend()
    plt.show()

    # Calculate and print R-squared

    validation_score = r2_score(param_data, fit_function(temp_data, *coefficients))
    print(f"R-squared for {param_column}: {validation_score:.4f}")

    return coefficients

"""
**Calculate and print parameter values for the new input temperature**"""

if __name__ == "__main__":

    data = pd.read_csv('/content/MAPbI3_Params.csv')

    # Specify the temperature column and parameter columns

    temperature_column = 'T'
    parameter_columns = ['R', 'C1', 'a1', 'R1', 'C2', 'a2','R2']

    # Allow the user to input a temperature and get predictions

    while True:
        input_temp = input("Enter a temperature value (or type 'exit' to end): ")
        if input_temp.lower() == 'exit':
            break

        input_temp = float(input_temp)

        # Perform curve fitting with a higher degree polynomial and a better initial guess

        for param_column in parameter_columns:
            coefficients = fit_and_print_equation(data, temperature_column, param_column)


            prediction = fit_function(input_temp, *coefficients)
            print(f"Prediction for {param_column} at temperature {input_temp}: {prediction}")

"""**Calculation of EIS data**"""

# identify equivalent circuit parameter and use as input parameter

def calculate_total_impedance(r1, r2, cpe1, a1, r3, cpe2, a2, frequency):
    j = complex(0, 1)

    # Impedance of constant phase elements
    impedance_cpe1 = 1 / (((j * 2 * np.pi *  (frequency)) ** a1) * cpe1)
    impedance_cpe2 = 1 / (((j * 2 * np.pi * (frequency)) ** a2) * cpe2)

    # Impedance of resistors and combinations
    impedance_r2_cpe1 = 1 / (1 / r2 + 1 / impedance_cpe1)
    impedance_r3_cpe2 = 1 / (1 / r3 + 1 / impedance_cpe2)

    # Total impedance of the circuit
    total_impedance = r1 + impedance_r2_cpe1 + impedance_r3_cpe2

    return total_impedance.real, total_impedance.imag

"""**Save EIS values as excel file**"""

def save_impedance_data_to_excel(output_file, frequencies, real_impedance, imaginary_impedance):
    data = {'Frequency (Hz)': frequencies, 'Real Impedance (Ohms)': real_impedance, 'Imaginary Impedance (Ohms)': imaginary_impedance}
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)

"""**Calculation using extracted parameter**"""

# Set your parameters
r1 =  7800 # Replace with actual resistance value for R1
r2 =  72252.7998932879 # Replace with actual resistance value for R2
cpe1 = 1.0301538737738922e-06  # Replace with actual values for constant phase element 1
r3 =   102261.95090336667 # Replace with actual resistance value for R3
cpe2 =   4.2089600021017235e-10 # Replace with actual values for constant phase element 2
a1 =  0.78 #alpha 1
a2 =  0.978 #alpha 2


# Frequency range from 1 Hz to 1 MHz
frequencies = np.logspace(0, 6, num=125)

# Calculate impedance for each frequency
real_impedance, imaginary_impedance = zip(*[calculate_total_impedance(r1, r2, cpe1, a1, r3, cpe2, a2, f) for f in frequencies])

# Save the results to an Excel file
output_file = 'impedance_results8.xlsx'
save_impedance_data_to_excel(output_file, frequencies, real_impedance, imaginary_impedance)

print(f"Impedance data saved to {output_file}")

"""**T-SNE Data Visulization**"""

import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.manifold import TSNE

data = pd.read_csv("/content/Synthetic_Training_Data.csv")

X = data.iloc[: , :4]
y = data.iloc[: , 4:]
y_list = np.ravel(y).tolist()

tsne = TSNE(n_components=2, random_state=42)
X_embedded = tsne.fit_transform(X)

plt.figure(figsize=(10, 8))
plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_list, cmap=plt.cm.Set1)
plt.title("t-SNE Visualization of Iris Dataset")
plt.xlabel("t-SNE 1")
plt.ylabel("t-SNE 2")
plt.colorbar()

plt.savefig("t-sne-Experimentaldata.png", dpi=900)
plt.show()

"""**Feature Visulization**"""

fig, axs = plt.subplots(2, 2, figsize=(12, 10))

for i, ax in enumerate(axs.flat):
    ax.scatter(X.iloc[:, i], y, color='blue')
    ax.set_title(f'Feature {i+1}')
    ax.set_xlabel(f'Feature {i+1}')
    ax.set_ylabel('Target')

plt.tight_layout()
plt.show()

"""**Visual Correlation Matrix**"""

import seaborn as sns

target_col = df.columns[-1]

sns.pairplot(df)
plt.show()

correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

for col in df.columns[:-1]:  # Exclude the target variable
    plt.figure(figsize=(8, 6))
    sns.scatterplot(data=df, x=col, y=target_col)
    plt.title(f'{col} vs {target_col}')
    plt.xlabel(col)
    plt.ylabel(target_col)
    plt.show()
    plt.savefig("Experimentaldata.png", dpi=900)

"""# **Machine Learning model**

**Random Forest Regressor**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt

data = pd.read_csv("/content/Synthetic_Training_Data.csv")

X = data.iloc[: , :4]  # Features
y = data.iloc[: , 4:] # Target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=41)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = RandomForestRegressor(n_estimators=41, min_samples_split = 7,  random_state=44, min_impurity_decrease = 15)
model.fit(X_train_scaled, y_train)

y_pred = model.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mape = mean_absolute_percentage_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("Mean Absolute % Error:", mape)
print("R2 Score:", r2)

"""**Correlation Matrix**"""

import seaborn as sns
plt.figure(figsize=(8, 6))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

"""**Scatter Plot**"""

row_number_test = range(1, len(y_pred) + 1)
plt.figure(figsize=(8,6))
plt.scatter(row_number_test, y_test, color = 'blue')
plt.scatter(row_number_test, y_pred, color = 'black')
plt.legend()
plt.show()

"""**Test Model on Unseen Temperature Data**"""

df = pd.read_csv('/content/-5_Br.csv')
X = df.iloc[: , :4]  # Features
y = df.iloc[: , 4:] # Target

scaler = StandardScaler()
X_new = scaler.fit_transform(X)

predict_data = model.predict(X_new)
print(predict_data)
row_number_test = range(1, len(X_new) + 1)
plt.figure(figsize=(8,6))
plt.scatter(row_number_test, predict_data, color = 'blue')
plt.scatter(row_number_test, y, color = 'red')
plt.legend()
plt.show()

p_data = pd.DataFrame(predict_data, columns = ['y_-7'])
p_data.to_csv('y_-7.csv', index = False)

mse = mean_squared_error(y, predict_data)
mae = mean_absolute_error(y, predict_data)
mape = mean_absolute_percentage_error(y,predict_data)
r2 = r2_score(y, predict_data)
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)
print("Mean Absolute % Error:", mape)
print("R2 Score:", r2)

"""**Performance on different regressor model**"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score

data = pd.read_csv("/content/Synthetic_Training_Data.csv")

# 2. Preprocess the data
# (You may need to handle missing values, encode categorical variables, etc.)

# 3. Split the dataset into training and testing sets
X = data.iloc[: , :4]  # Features
y = data.iloc[: , 4:] # Target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=41)

# 4. Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
y_train_scaled = scaler.fit_transform(y_train)
y_test_scaled = scaler.transform(y_test)

# Linear Regression
lr = LinearRegression()
lr.fit(X_train_scaled, y_train_scaled)
lr_pred = lr.predict(X_test_scaled)
lr_mse = mean_squared_error(y_test_scaled, lr_pred)
lr_r2 = r2_score(y_test_scaled, lr_pred)

# Ridge Regression
ridge = Ridge(alpha=0.5)
ridge.fit(X_train_scaled, y_train_scaled)
ridge_pred = ridge.predict(X_test_scaled)
ridge_mse = mean_squared_error(y_test_scaled, ridge_pred)
ridge_r2 = r2_score(y_test_scaled, ridge_pred)

# Lasso Regression
lasso = Lasso(alpha=0.1)
lasso.fit(X_train_scaled, y_train_scaled)
lasso_pred = lasso.predict(X_test_scaled)
lasso_mse = mean_squared_error(y_test_scaled, lasso_pred)
lasso_r2 = r2_score(y_test_scaled, lasso_pred)

# Decision Tree Regression
tree = DecisionTreeRegressor(max_depth=5)
tree.fit(X_train_scaled, y_train_scaled)
tree_pred = tree.predict(X_test_scaled)
tree_mse = mean_squared_error(y_test_scaled, tree_pred)
tree_r2 = r2_score(y_test_scaled, tree_pred)

# Comparing performance
models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression', 'Decision Tree Regression', 'Random Forest Regression']
mse_scores = [lr_mse, ridge_mse, lasso_mse, tree_mse, rf_mse]
r2_scores = [lr_r2, ridge_r2, lasso_r2, tree_r2, rf_r2]
results_df = pd.DataFrame({'Model': models, 'Mean Squared Error': mse_scores, 'r2 score':r2_scores})
print(results_df)